#!/usr/bin/env perl

use strict;
use warnings;
use 5.012;

use autodie;
use Carp;
use DBI;
use Digest::MD5;
use English;
#use File::stat;
use Fcntl qw/:DEFAULT :flock :seek/;
use File::Temp qw/tempdir/;
use Getopt::Long;
use List::MoreUtils qw/any/;
use Net::FTP;
use Pod::Usage;
use Sys::Syslog qw/:standard :macros/;
use Try::Tiny;

use constant NAME    => 'ncbi_blast_update';
use constant VERSION => '0.301';

use constant SQL_DB => 'ncbi_blast.sqlite';
use constant LOCKFILE => '/var/tmp/ncbi_blast_update.pid';

my $server        = 'ftp.ncbi.nlm.nih.gov';
my $remote_path   = '/blast/db';
my $passive       = 1;
my $timeout       = 600;
my $verbose       = 0;
my $list_dbs      = 0;
my $print_version = 0;
my $print_help    = 0;
my $local_path    = '';
my $clean         = 0;
my $log           = 0;
my $tries         = 3;
my @dbs;

my $exit_status   = 0;

my $res = GetOptions(
    'server=s'      => \$server,
    'remote_path=s' => \$remote_path,
    'passive'       => \$passive,
    'timeout=i'     => \$timeout, # in seconds
    'verbose'       => \$verbose,
    'list'          => \$list_dbs,
    'version'       => \$print_version,
    'db=s'          => \@dbs,
    'local_path=s'  => \$local_path,
    'clean'         => \$clean,
    'syslog'        => \$log,
    'attempts=i'    => \$tries,
    'man'           => sub{ pod2usage(-verbose => 2); },
    'help'          => sub{ pod2usage(-verbose => 2); },
);

# Handle version requests
if ($print_version) {
    say 'This is ' , NAME, ' v' , VERSION;
    say 'Copyright 2014-2018 Jeremy Volkening (jeremy@base2bio.com)';
    say 'Licensed under the GNU GPLv3';
    exit;
}

# open syslog connection
if ($log) {
    openlog( NAME, 'perror', LOG_LOCAL1 )
        or croak "Error opening syslog connection: $!";
}

chdir $local_path
    or log_die( LOG_ERR, "Can't cd to $local_path" );

# check PID file to avoid multiple instances
take_lock();

# handle various signals by terminating
$SIG{TERM} = $SIG{KILL} = $SIG{INT} = \&clean_quit;

#open local sqlite db
my $dbh = initialize_sql();

# Parse comma-sep db string
@dbs = split( /,/, join(',', @dbs) );


# build list of remote database structures
my $ftp = new_conn();
my @entries = $ftp->dir;
$ftp->quit;
$ftp = undef;
my $remote_dbs = parse_remote(@entries);

# check all requested DBs against remote tree
for my $db (@dbs) {
    if (! defined $remote_dbs->{$db}) {
        say STDERR "$db is not a valid database identifier.";
        syslog(LOG_NOTICE, "Invalid database name: $db")
            if ($log);
        $exit_status = 1;
        $list_dbs    = 1;
    }
}

# handle request for DB list
if ($list_dbs) {
    my @sorted = sort {$a cmp $b} keys %{ $remote_dbs };
    say "\nAvailable databases:\n-------------------";
    say $_ for @sorted;
    say '-------------------';
    clean_quit();
}

# Attempt to check and download each requested database
for my $db (@dbs) {

    try {
        download($db);
    }
    catch {
        carp $_
            if ($verbose);
        syslog(LOG_NOTICE, "Download of $db FAILED: $_")
            if ($log);
        $exit_status = 2;
    };

}

clean_quit();

sub process_remote_db {

    my ($db_name) = @_;

    my $tree;

    for my $upstream (values %{ $remote_dbs->{$db_name} }) {

        my $fn_parent = $upstream->{db};
        my $fn_md5    = $upstream->{md5};
        croak "Invalid characters in remote filename $fn_parent"
            if  ($fn_parent =~ /[^\w\.]/);
        $tree->{$fn_parent} = {
            name => $fn_parent,
            md5  => fetch_md5( $fn_md5 ),
        }

    }

    return $tree;

}


sub download {

    my ($db_name) = @_;

    # move to working directory (must be on same volume as $local_path for
    # hard-linking to work)
    my $temp_dir = tempdir(
        CLEANUP => 1,
        DIR     => $local_path,
    );
    chdir $temp_dir; 

    # fetch hash of remote parents for db
    my $remote_parents = process_remote_db( $db_name );

    # fetch hash of local parents for db
    my $local_parents = $dbh->selectall_hashref(
        "SELECT name,md5 FROM compressed WHERE database=?",
        'name',
        {},
        $db_name,
    );

    # fetch hash of local chlidren for db
    my $local_children = $dbh->selectall_hashref(
        "SELECT name,parent,md5 FROM files WHERE database=?",
        'name',
        {},
        $db_name,
    );

    my $new_downloads = 0; # track number of newly fetched files for log

    my %needed;
    my %decompressed;

    try {

        local $SIG{TERM} = local $SIG{INT} = sub {
            croak "Received INT/TERM signal while fetching $db_name";
        };

        for my $parent (keys %{$remote_parents}) {

            # check existing file records against upstream MD5s
            if (! defined $local_parents->{$parent}
             || $remote_parents->{$parent}->{md5}
              ne $local_parents->{$parent}->{md5}) {
                $needed{$parent} = $remote_parents->{$parent}->{md5};
            }
            
            # perform download if necessary
            if ($needed{$parent}) {

                my $remaining = $tries;

                TRY:
                for (1..$tries) {

                    my $success = try {

                        say STDERR "fetching $parent"
                            if ($verbose);

                        $remaining = $tries - $_;

                        my $ftp = new_conn();
                        $ftp->get($parent) or croak "Error fetching $parent: "
                            . $ftp->message;
                        $ftp->quit;
                        $ftp = undef;

                        open my $fh, '<', $parent;
                        binmode $fh;
                        say STDERR "calculating MD5 on $parent"
                            if ($verbose);
                        my $md5 = Digest::MD5->new->addfile($fh)->hexdigest;
                        close $fh;

                        croak "MD5 mismatch on downloaded $parent"
                            if ($md5 ne $remote_parents->{$parent}->{md5});

                        # If we get here, the file downloaded successfully
                        ++$new_downloads;

                        say STDERR "decompressing $parent"
                            if ($verbose);
                        # Be careful here, $parent must have been sanitized
                        my @files = split /\s+/, `tar -xvzf $parent`
                            or croak "Error decompressing $parent: $@";
                        unlink $parent;
                        CHILD:
                        for my $child (@files) {

                            # ignore files such as 'taxdb.bt*', since they may
                            # not be unique to the database. These files
                            # should be retrieve separately.
                            if ($child !~ /^$db_name/) {
                                unlink $child;
                                next CHILD;
                            }

                            open my $fh, '<', $child;
                            binmode $fh;
                            say STDERR "calculating MD5 on $child"
                                if ($verbose);
                            my $md5 = Digest::MD5->new->addfile(
                                $fh,
                            )->hexdigest;
                            close $fh;
                            $decompressed{$child} = {
                                name   => $child,
                                parent => $parent,
                                md5    => $md5,
                            };
                        }

                        return 1;

                    } catch {
                        croak "Failed to download $parent: $_"
                            if ($remaining < 1);
                        unlink $parent if (-e $parent);
                        return 0;
                    };

                    last TRY if $success;

                }
            }
            # if matching file exists, hard link all decompressed files
            else {
                for my $file (
                    grep {$_->{parent} eq $parent}
                    values %{$local_children}
                ) {
                    my $fn = $file->{name};
                    croak "Error hardlinking: can't find $fn"
                        if (! -e "$local_path/$fn");
                    say STDERR "hard linking $fn"
                        if ($verbose);
                    link "$local_path/$fn" => $fn; #hard link existing file
                }
            }

        }
    }
    catch {
        carp $_;
        say STDERR "removing $temp_dir\n"
            if ($verbose);
        chdir $temp_dir; # should be here already
        unlink glob '*';
        chdir '..';
        rmdir $temp_dir;
        clean_quit();
    };

    #If we get this far, all downloads and decompressions were successful

    chdir $local_path;

    #Delete old files and database entries
    for my $fetched (keys %needed) {
        if (-e $fetched) {
            say STDERR "deleting local file $fetched\n"
                if $verbose;
            unlink $fetched;
        }
        say STDERR "removing compressed db entry for $fetched"
            if $verbose;
        $dbh->do("DELETE FROM compressed WHERE name=?", {}, $fetched)
            or croak "Error deleting $fetched from compressed table: $@";

        for my $child (keys %{$local_children}) {
            say STDERR "removing files db entry for $child"
                if $verbose;
            $dbh->do("DELETE FROM files WHERE name=?", {}, $child)
                or croak "Error deleting $child from files table: $@";
        }
    }

    #Copy files
    my @files = glob "$temp_dir/*";
    for (@files) {
        my $newfile = $_;
        $newfile =~ s/^.+\///;
        say STDERR "moving $_ to $newfile"
            if ($verbose);
        rename( $_ => $newfile );
        unlink $_ if (-e $_);
    }
    say STDERR "removing $temp_dir"
        if ($verbose);
    rmdir $temp_dir;
    syslog( LOG_INFO,
      "Successfully updated $db_name ($new_downloads new files)")
        if ($log);

    #Add new database entries
    for my $parent (keys %needed) {
        $dbh->do("INSERT INTO compressed VALUES (?,?,?,?)", {},
            $parent,
            $db_name,
            $needed{$parent},
            time(),
        ) or croak "Error updating compressed table: $@";
    }
    for my $file (keys %decompressed) {
        $dbh->do("INSERT INTO files VALUES (?,?,?,?,?)", {},
            $file,
            $db_name,
            $decompressed{$file}->{parent},
            $decompressed{$file}->{md5},
            time(),
        ) or croak "Error updating files table: $@";
    }

    # Final database cleanup

    #Delete orphans (parent and child files no longer in upstream database)
    for my $parent (keys %{$local_parents}) {
        if (! defined $remote_parents->{$parent}) {

            # remove parent
            say STDERR "removing compressed db entry for orphan $parent"
                if $verbose;
            $dbh->do("DELETE FROM compressed WHERE name=?", {},
                $parent,
            ) or croak "Error deleting orphan $parent from compressed: $@";

            # remove children
            for my $child (
                grep {$_->{parent} eq $parent}
                values %{$local_children}
            ) {
                my $name = $child->{name};
                say STDERR "removing files db entry for orphan $name"
                    if $verbose;
                $dbh->do("DELETE FROM files WHERE name=?", {}, $name)
                    or croak "Error deleting orphan $name from files: $@";
            }
        }
    }


}
            

sub parse_remote {

    my @entries = @_;

    # $struct is array ref with [db_filename, md5_filename]
    my $struct = {};

    # parse db file structure
    FILE:
    for my $entry (@entries) {
        next FILE if ($entry =~ /^d/);     #ignore directories
        next FILE if ($entry !~ /tar.gz/); #ignore misc files
        my $fn = $entry;
        $fn =~ s/.+\s//; # remove everything to last space, leave filename

        # validate naming scheme (also prevents injection attack)
        if ($fn =~ /(\w+)(?:\.0*(\d+))?\.tar\.gz(\.md5)?/) {
            my $basename = $1;
            my $index    = $2 // 0;
            my $slot     = $3 ? 'md5' : 'db';
            $struct->{$basename}->{$index}->{$slot} = $fn;
        }
        else {
            log_die (LOG_ERR, "unexpected remote filename: $fn\n");
        }
    }

    # validate structure (all dbs should have a complete and sequential set of
    # indices (0,1,2,...,n) and both filename and md5sum
    for my $db (keys %{$struct}) {

        my @indices = sort {$a <=> $b} keys %{ $struct->{$db} };
        my $was_warned = 0;
        for (0..$#indices) {
            # test for complete sequences
            if (! $was_warned && $_ != $indices[$_]) { 
                syslog(LOG_NOTICE, "Inconsistent structure for $db: @indices")
                    if ($log);
                $was_warned = 1;
            }
            # test for complete file/md5 pairs
            if ( ! defined $struct->{$db}->{$indices[$_]}->{db}
              || ! defined $struct->{$db}->{$indices[$_]}->{md5} ) {
                log_die(LOG_ERR, "Incomplete file pair for $db:$_\n");
            }
        }
            
    }

    return $struct;

}

sub initialize_sql {

    say STDERR "Initializing database"
        if ($verbose);
    unlink SQL_DB if ($clean);
    my $init = 1 if (! -e SQL_DB);
    my $dbh = DBI->connect("dbi:SQLite:dbname=" . SQL_DB, "", "");
    if ($init) {
        $dbh->do( "CREATE TABLE compressed ( "
            . "name VARCHAR(255) PRIMARY KEY, "
            . "database VARCHAR(64), "
            . "md5 CHAR(16), "
            . "download_time INTEGER"
            . ")"
        );
        $dbh->do( "CREATE TABLE files ( "
            . "name VARCHAR(255) PRIMARY KEY, "
            . "database VARCHAR(64), "
            . "parent VARCHAR(255), "
            . "md5 CHAR(16), "
            . "creation_time INTEGER"
            . ")"
        );
    }
    return $dbh;

}

sub fetch_md5 {

    my $filename = shift;
    my $remote_md5;
    my $ftp = new_conn();
    my $conn = $ftp->retr($filename)
        or croak "RETR failed for $filename: $@ $!";
    my $rcvd = $conn->read($remote_md5, 32);
    $conn->close();
    $ftp->quit;
    croak "failed to read md5 hash from $filename\n"
        if ($rcvd != 32);
    say STDERR "Fetched MD5 from $filename: $remote_md5"
        if $verbose;
    return $remote_md5;
    
}

sub clean_quit {

    closelog() if ($log);
    unlink LOCKFILE;
    exit $exit_status;

} 

sub log_die {

    my ($level, $msg) = @_;
    if ($log) {
        syslog($level, $msg);
        closelog();
    }
    $ftp->quit if (defined $ftp);
    carp $msg;
    $exit_status = 2;
    clean_quit();

}

sub take_lock {

    sysopen my $fh, LOCKFILE, O_RDWR|O_CREAT
        or log_die(LOG_ERR, "lockfile open: $!");
    flock $fh => LOCK_EX or log_die(LOG_ERR, "flock error: $!");

    my $pid = <$fh>;
    if (defined $pid) {
        chomp $pid;
        if (kill ZERO => $pid) { #if process exists
            close $fh;
            log_die(LOG_INFO, "skipped update: detected running process");
        }
    }

    sysseek  $fh, 0, SEEK_SET or log_die(LOG_ERR, "sysseek: $!");
    truncate $fh, 0,          or log_die(LOG_ERR, "truncate: $!");
    say     {$fh} $PID        or log_die(LOG_ERR, "say: $!");
    close    $fh              or log_die(LOG_ERR, "close: $!");

}

sub new_conn {

    my $ftp = Net::FTP->new(
        $server,
        Passive => $passive,
        Timeout => $timeout,
    ) or log_die( LOG_ERR, "Unable to connect: $@" );
    $ftp->login('anonymous','anonymous')
        or log_die( LOG_ERR, "Unable to login: " . $ftp->message );
    $ftp->cwd($remote_path)
        or log_die( LOG_ERR, "Unable to cwd: " . $ftp->message );
    $ftp->binary()
        or log_die( LOG_ERR, "Unable to switch to binary: " . $ftp->message );
       return $ftp;
}


__END__

=head1 NAME

ncbi_blast_update - intelligent management of updates to local BLAST databases
from remote servers

=head1 SYNOPSIS

ncbi_blast_update [options] --db db1,db2,etc --local_path path/to/local/db

=head1 OPTIONS

=over 4

=item B<--server I<string>>

FQDN of FTP server to use

=item B<--remote_path I<string>>

Absolute path on the FTP server to the BLAST database files

=item B<--local_path>

Full path to local directory where BLAST database files are stored.  The
SQLite database will also be written to this directory if it does not exist.
The user running the update must have full read/write permissions on this
directory.

=item B<--passive>

Use passive FTP. This is often necessary when downloading from behind a
firewall (default: TRUE).

=item B<--timeout> I<integer>

Set FTP timeout, in seconds (default: 600)

=item B<--attempts I<integer>>

Number of times to attempt a download before giving up (default: 3)

=item B<--list>

Don't attempt any downloads - just query the remote server and print a list
of all databases available for download.

=item B<--clean>

Creates fresh SQLite database before commencing download (overwriting existing
database file if necessary). Use with caution - this option will wipe out the
download history and force a new download of all requested databases. It will
not delete BLAST files on disk, although it is recommended to do so before
running this command to keep things clean and sychronized.

=item B<--db> I<string>

Comma-separated list of database names to check/update. Example: 'nt,nr'

=item B<--syslog>

Send status and error messages to the syslog daemon, if running

=item B<--verbose>

Print additional warnings and status messages to STDERR

=item B<--version>

Print sofware name, version, and license info and exit

=back

=head1 DESCRIPTION

This program handles updating and tracking of currently installed preformatted
NCBI BLAST databases. It tracks local versions using SQLite, and compares
MD5 sums between remote files and records of previous downloads. It only
downloads database files whose MD5 sums have changed, and thus in theory is
capable of incremental updates, although experience suggests that NCBI makes
changes to all files in a database during updates that result in different
MD5 sums and therefore triggers new downloads of all database files. New
downloads are first saved to a temporary directory, and therefore the program
is relatively tolerant to errors during download or decompression in that it
does not remove any existing files from disk until all previous steps have
completed successfully.

The suggested way to use this script is as a cron job (daily, weekly, monthly,
etc, as desired). In this case, syslog logging is implemented to integrate
tracking of success or failure into standard system monitoring and reporting.

=head1 CAVEATS

Currently, the software does not make much effort to handle orphaned files
(e.g. files in the local BLAST directory that, for whatever reason, are not
tracked in the current sqlite database. This can be convenient, since it
allows non-NCBI databases to co-exist in the same directory without fear of
being inadvertently removed. The only situation in which the software will
remove orphaned files is if they are currently listed in the SQLite database
but no longer exist remotely.

=head1 AUTHOR

Jeremy Volkening (jdv@base2bio.com)

=head1 COPYRIGHT AND LICENSE

Copyright 2014-2018 Jeremy Volkening

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.

=cut
